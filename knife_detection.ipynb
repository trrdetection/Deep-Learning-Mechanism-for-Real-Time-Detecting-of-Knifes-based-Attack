{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knife_detection_SSD_MobileNet_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QcRzAJNUI1Ic",
        "ehxyzOi5JJlj",
        "-FAMUpuzJSX9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjaLsAwoRN8"
      },
      "source": [
        "## Choosing a pre-training model\r\n",
        "The model used for this project is `ssd_inception_v2_coco`.\r\n",
        "\r\n",
        "Because the interestes of this project is to interfere on real time video, i am chosing a model that has a high inference speed `(ms)` with relativly high `mAP` on COCO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pHQCu7Wzui1"
      },
      "source": [
        "MODELS_CONFIG = {\r\n",
        "    'ssd_mobilenet_v2': {\r\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\r\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\r\n",
        "    }\r\n",
        "    'ssd_inception_v2': {\r\n",
        "        'model_name': 'ssd_inception_v2_coco_2018_01_28',\r\n",
        "        'pipeline_file': 'ssd_inception_v2_coco.config',\r\n",
        "    },\r\n",
        "}\r\n",
        "selected_model = 'ssd_mobilenet_v2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcRzAJNUI1Ic"
      },
      "source": [
        "## Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A_wCFMN1AIU"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 #downgrade\r\n",
        "\r\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\r\n",
        "\r\n",
        "!pip install -qq Cython contextlib2 pillow lxml matplotlib\r\n",
        "\r\n",
        "!pip install -qq pycocotools\r\n",
        "\r\n",
        "!pip install tf_slim\r\n",
        "\r\n",
        "!pip install lvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehxyzOi5JJlj"
      },
      "source": [
        "## General Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxpjZ-oc0kke"
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "import pandas as pd\r\n",
        "import csv\r\n",
        "import cv2\r\n",
        "\r\n",
        "from collections import namedtuple, OrderedDict\r\n",
        "import io\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import urllib.request\r\n",
        "import tarfile\r\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNrblACH0n9t"
      },
      "source": [
        "#to check the tensorflow\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FAMUpuzJSX9"
      },
      "source": [
        "## Connecting to google drive\r\n",
        "\r\n",
        "object_detection directory is at root"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HADkHVyn1bIZ"
      },
      "source": [
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14RuOfCz1oEO"
      },
      "source": [
        "%cd /gdrive/'My Drive'/object_detection\r\n",
        "!mkdir knife_detection\r\n",
        "%cd knife_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jb78ylVJhmb"
      },
      "source": [
        "##Importing and organizing images and annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdPLxr_r2hCQ"
      },
      "source": [
        "!git clone https://github.com/ari-dasci/OD-WeaponDetection.git "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THIx0XRz8uGW"
      },
      "source": [
        "!mkdir data\r\n",
        "\r\n",
        "!mkdir data/images data/train_labels data/test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6es42H7Q9n_K"
      },
      "source": [
        "!mv OD-WeaponDetection/Knife_detection/Images/* data/images\r\n",
        "!mv OD-WeaponDetection/Knife_detection/annotations/* data/train_labels\r\n",
        "\r\n",
        "!ls data/train_labels/* | sort -R | head -400 | xargs -I{} mv {} data/test_labels\r\n",
        "!ls -1 data/train_labels/ | wc -l\r\n",
        "!ls -1 data/test_labels/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrNTyIrgwKNh"
      },
      "source": [
        "%cd /gdrive/My Drive/object_detection/knife_detection/data\r\n",
        "\r\n",
        "images_extension = 'jpg'\r\n",
        "\r\n",
        "def xml_to_csv(path):\r\n",
        "  classes_names = []\r\n",
        "  xml_list = []\r\n",
        "\r\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\r\n",
        "    tree = ET.parse(xml_file)\r\n",
        "    root = tree.getroot()\r\n",
        "    for member in root.findall('object'):\r\n",
        "      classes_names.append(member[0].text)\r\n",
        "      value = (root.find('filename').text,\r\n",
        "               int(root.find('size')[0].text),\r\n",
        "               int(root.find('size')[1].text),\r\n",
        "               member[0].text,\r\n",
        "               int(member[4][0].text),\r\n",
        "               int(member[4][1].text),\r\n",
        "               int(member[4][2].text),\r\n",
        "               int(member[4][3].text))\r\n",
        "      xml_list.append(value)\r\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\r\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name) \r\n",
        "  classes_names = list(set(classes_names))\r\n",
        "  classes_names.sort()\r\n",
        "  return xml_df, classes_names\r\n",
        "\r\n",
        "for label_path in ['train_labels', 'test_labels']:\r\n",
        "  image_path = os.path.join(os.getcwd(), label_path)\r\n",
        "  xml_df, classes = xml_to_csv(label_path)\r\n",
        "  xml_df.to_csv(f'{label_path}.csv', index=None)\r\n",
        "  print(f'Successfully converted {label_path} xml to csv.')\r\n",
        "\r\n",
        "label_map_path = os.path.join(\"label_map.pbtxt\")\r\n",
        "\r\n",
        "pbtxt_content = \"\"\r\n",
        "\r\n",
        "for i, class_name in enumerate(classes):\r\n",
        "    pbtxt_content = (\r\n",
        "        pbtxt_content\r\n",
        "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n    display_name: 'knife'\\n }}\\n\\n\".format(i + 1, class_name)\r\n",
        "    )\r\n",
        "pbtxt_content = pbtxt_content.strip()\r\n",
        "with open(label_map_path, \"w\") as f:\r\n",
        "    f.write(pbtxt_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162i4lZrvmAq"
      },
      "source": [
        "!cat label_map.pbtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW8nkhlt14ly"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPAjPUrV2h26"
      },
      "source": [
        "%cd /gdrive/My Drive/object_detection/knife_detection/data\r\n",
        "\r\n",
        "images_path = 'images'\r\n",
        "\r\n",
        "for CSV_FILE in ['train_labels.csv', 'test_labels.csv']:\r\n",
        "  with open(CSV_FILE, 'r') as fid:  \r\n",
        "      print('[*] Checking file:', CSV_FILE) \r\n",
        "      file = csv.reader(fid, delimiter=',')\r\n",
        "      first = True \r\n",
        "      cnt = 0\r\n",
        "      error_cnt = 0\r\n",
        "      error = False\r\n",
        "      for row in file:\r\n",
        "          if error == True:\r\n",
        "              error_cnt += 1\r\n",
        "              error = False         \r\n",
        "          if first == True:\r\n",
        "              first = False\r\n",
        "              continue     \r\n",
        "          cnt += 1      \r\n",
        "          name, width, height, xmin, ymin, xmax, ymax = row[0], int(row[1]), int(row[2]), int(row[4]), int(row[5]), int(row[6]), int(row[7])     \r\n",
        "          path = os.path.join(images_path, name)\r\n",
        "          img = cv2.imread(path)         \r\n",
        "          if type(img) == type(None):\r\n",
        "              error = True\r\n",
        "              print('Could not read image', img)\r\n",
        "              continue     \r\n",
        "          org_height, org_width = img.shape[:2]     \r\n",
        "          if org_width != width:\r\n",
        "              error = True\r\n",
        "              print('Width mismatch for image: ', name, width, '!=', org_width)     \r\n",
        "          if org_height != height:\r\n",
        "              error = True\r\n",
        "              print('Height mismatch for image: ', name, height, '!=', org_height) \r\n",
        "          if xmin > org_width:\r\n",
        "              error = True\r\n",
        "              print('XMIN > org_width for file', name)  \r\n",
        "          if xmax > org_width:\r\n",
        "              error = True\r\n",
        "              print('XMAX > org_width for file', name)\r\n",
        "          if ymin > org_height:\r\n",
        "              error = True\r\n",
        "              print('YMIN > org_height for file', name)\r\n",
        "          if ymax > org_height:\r\n",
        "              error = True\r\n",
        "              print('YMAX > org_height for file', name)\r\n",
        "          if error == True:\r\n",
        "              print('Error for file: %s' % name)\r\n",
        "              print()\r\n",
        "      print()\r\n",
        "      print('Checked %d files and realized %d errors' % (cnt, error_cnt))\r\n",
        "      print(\"-----\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49SRu4y5ZpX3"
      },
      "source": [
        "##Downloading the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv_5wt9lJVyX"
      },
      "source": [
        "%cd /gdrive/My Drive/object_detection/knife_detection\r\n",
        "!git clone --q https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7WDkQ8OKK4u"
      },
      "source": [
        "%cd /gdrive/My Drive/object_detection/knife_detection/models/research\r\n",
        "\r\n",
        "!protoc object_detection/protos/*.proto --python_out=.\r\n",
        "\r\n",
        "os.environ['PYTHONPATH'] += ':/gdrive/My Drive/object_detection/knife_detection/models/research/:/gdrive/My Drive/object_detection/knife_detection/models/research/slim/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykMzlTgtKdih"
      },
      "source": [
        "!python3 object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NjH6pUAatt1"
      },
      "source": [
        "##Generating tf records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3tA0opTNCZk"
      },
      "source": [
        "from object_detection.utils import dataset_util\r\n",
        "%cd /gdrive/My Drive/object_detection/knife_detection/models/\r\n",
        "\r\n",
        "DATA_BASE_PATH = '/gdrive/My Drive/object_detection/knife_detection/data/'\r\n",
        "image_dir = DATA_BASE_PATH +'images/'\r\n",
        "\r\n",
        "def class_text_to_int(row_label):\r\n",
        "\t\tif row_label == 'knife':\r\n",
        "\t\t\t\treturn 1\r\n",
        "\t\telse:\r\n",
        "\t\t\t\tNone\r\n",
        "\r\n",
        "\r\n",
        "def split(df, group):\r\n",
        "\t\tdata = namedtuple('data', ['filename', 'object'])\r\n",
        "\t\tgb = df.groupby(group)\r\n",
        "\t\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\r\n",
        "\r\n",
        "def create_tf_example(group, path):\r\n",
        "\t\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\r\n",
        "\t\t\t\tencoded_jpg = fid.read()\r\n",
        "\t\tencoded_jpg_io = io.BytesIO(encoded_jpg)\r\n",
        "\t\timage = Image.open(encoded_jpg_io)\r\n",
        "\t\twidth, height = image.size\r\n",
        "\r\n",
        "\t\tfilename = group.filename.encode('utf8')\r\n",
        "\t\timage_format = b'jpg'\r\n",
        "\t\txmins = []\r\n",
        "\t\txmaxs = []\r\n",
        "\t\tymins = []\r\n",
        "\t\tymaxs = []\r\n",
        "\t\tclasses_text = []\r\n",
        "\t\tclasses = []\r\n",
        "\r\n",
        "\t\tfor index, row in group.object.iterrows():\r\n",
        "\t\t\t\txmins.append(row['xmin'] / width)\r\n",
        "\t\t\t\txmaxs.append(row['xmax'] / width)\r\n",
        "\t\t\t\tymins.append(row['ymin'] / height)\r\n",
        "\t\t\t\tymaxs.append(row['ymax'] / height)\r\n",
        "\t\t\t\tclasses_text.append(row['class'].encode('utf8'))\r\n",
        "\t\t\t\tclasses.append(class_text_to_int(row['class']))\r\n",
        "\r\n",
        "\t\ttf_example = tf.train.Example(features=tf.train.Features(feature={\r\n",
        "\t\t\t\t'image/height': dataset_util.int64_feature(height),\r\n",
        "\t\t\t\t'image/width': dataset_util.int64_feature(width),\r\n",
        "\t\t\t\t'image/filename': dataset_util.bytes_feature(filename),\r\n",
        "\t\t\t\t'image/source_id': dataset_util.bytes_feature(filename),\r\n",
        "\t\t\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n",
        "\t\t\t\t'image/format': dataset_util.bytes_feature(image_format),\r\n",
        "\t\t\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\r\n",
        "\t\t\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\r\n",
        "\t\t\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\r\n",
        "\t\t\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\r\n",
        "\t\t\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n",
        "\t\t\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n",
        "\t\t}))\r\n",
        "\t\treturn tf_example\r\n",
        "\r\n",
        "for csv in ['train_labels', 'test_labels']:\r\n",
        "  writer = tf.io.TFRecordWriter(DATA_BASE_PATH + csv + '.record')\r\n",
        "  path = os.path.join(image_dir)\r\n",
        "  examples = pd.read_csv(DATA_BASE_PATH + csv + '.csv')\r\n",
        "  grouped = split(examples, 'filename')\r\n",
        "  for group in grouped:\r\n",
        "      tf_example = create_tf_example(group, path)\r\n",
        "      writer.write(tf_example.SerializeToString())\r\n",
        "    \r\n",
        "  writer.close()\r\n",
        "  output_path = os.path.join(os.getcwd(), DATA_BASE_PATH + csv + '.record')\r\n",
        "  print('Successfully created the TFRecords: {}'.format(DATA_BASE_PATH +csv + '.record'))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c77CDi2qR64i"
      },
      "source": [
        "%cd /gdrive/My Drive/object_detection/knife_detection/data\r\n",
        "!ls -lX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxM6RSNbbCAw"
      },
      "source": [
        "##Downloading base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LmCdrDGSAZm"
      },
      "source": [
        "%cd /gdrive/My Drive/object_detection/knife_detection/models/research\r\n",
        "\r\n",
        "# Name of the object detection model to use.\r\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\r\n",
        "\r\n",
        "# Name of the pipline file in tensorflow object detection API.\r\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\r\n",
        "\r\n",
        "#selecting the model\r\n",
        "MODEL_FILE = MODEL + '.tar.gz'\r\n",
        "\r\n",
        "#creating the downlaod link for the model selected\r\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\r\n",
        "\r\n",
        "#the distination folder where the model will be saved\r\n",
        "fine_tune_dir = '/gdrive/My Drive/object_detection/knife_detection/models/research/pretrained_model'\r\n",
        "\r\n",
        "#checks if the model has already been downloaded\r\n",
        "if not (os.path.exists(MODEL_FILE)):\r\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\r\n",
        "\r\n",
        "#unzipping the file and extracting its content\r\n",
        "tar = tarfile.open(MODEL_FILE)\r\n",
        "tar.extractall()\r\n",
        "tar.close()\r\n",
        "\r\n",
        "# creating an output file to save the model while training\r\n",
        "os.remove(MODEL_FILE)\r\n",
        "if (os.path.exists(fine_tune_dir)):\r\n",
        "    shutil.rmtree(fine_tune_dir)\r\n",
        "os.rename(MODEL, fine_tune_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoDwAL0cSPNh"
      },
      "source": [
        "!echo {fine_tune_dir}\r\n",
        "%cd {fine_tune_dir}\r\n",
        "!ls -alh "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDTU4QWIbL8n"
      },
      "source": [
        "## Configuring the Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhO5rlVSz8-"
      },
      "source": [
        "CONFIG_BASE = \"/gdrive/My\\ Drive/object_detection/knife_detection/models/research/object_detection/samples/configs\"\r\n",
        "\r\n",
        "#path to the specified model's config file\r\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\r\n",
        "model_pipline = os.path.join(CONFIG_BASE, pipeline_file)\r\n",
        "model_pipline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdOoUAdAS3HX"
      },
      "source": [
        "%cd /gdrive/My\\ Drive/object_detection/knife_detection/models/research/object_detection/samples/configs/\r\n",
        "!cat ssd_mobilenet_v2_coco.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39MiSJsgTJWJ"
      },
      "source": [
        "#editing the configuration file to add the path for the TFRecords files, pbtxt,batch_size,num_steps,num_classes.\r\n",
        "# any image augmentation, hyperparemeter tunning (drop out, batch normalization... etc) would be editted here\r\n",
        "\r\n",
        "%%writefile ssd_mobilenet_v2_coco.config\r\n",
        "model {\r\n",
        "  ssd {\r\n",
        "    num_classes: 1 # number of classes to be detected\r\n",
        "    box_coder {\r\n",
        "      faster_rcnn_box_coder {\r\n",
        "        y_scale: 10.0\r\n",
        "        x_scale: 10.0\r\n",
        "        height_scale: 5.0\r\n",
        "        width_scale: 5.0\r\n",
        "      }\r\n",
        "    }\r\n",
        "    matcher {\r\n",
        "      argmax_matcher {\r\n",
        "        matched_threshold: 0.5\r\n",
        "        unmatched_threshold: 0.5\r\n",
        "        ignore_thresholds: false\r\n",
        "        negatives_lower_than_unmatched: true\r\n",
        "        force_match_for_each_row: true\r\n",
        "      }\r\n",
        "    }\r\n",
        "    similarity_calculator {\r\n",
        "      iou_similarity {\r\n",
        "      }\r\n",
        "    }\r\n",
        "    anchor_generator {\r\n",
        "      ssd_anchor_generator {\r\n",
        "        num_layers: 6\r\n",
        "        min_scale: 0.2\r\n",
        "        max_scale: 0.95\r\n",
        "        aspect_ratios: 1.0\r\n",
        "        aspect_ratios: 2.0\r\n",
        "        aspect_ratios: 0.5\r\n",
        "        aspect_ratios: 3.0\r\n",
        "        aspect_ratios: 0.3333\r\n",
        "      }\r\n",
        "    }\r\n",
        "    # all images will be resized to the below W x H.\r\n",
        "    image_resizer { \r\n",
        "      fixed_shape_resizer {\r\n",
        "        height: 300\r\n",
        "        width: 300\r\n",
        "      }\r\n",
        "    }\r\n",
        "    box_predictor {\r\n",
        "      convolutional_box_predictor {\r\n",
        "        min_depth: 0\r\n",
        "        max_depth: 0\r\n",
        "        num_layers_before_predictor: 0\r\n",
        "        #use_dropout: false\r\n",
        "        use_dropout: true # to counter over fitting. you can also try tweaking its probability below\r\n",
        "        dropout_keep_probability: 0.8\r\n",
        "        kernel_size: 1\r\n",
        "        box_code_size: 4\r\n",
        "        apply_sigmoid_to_scores: false\r\n",
        "        conv_hyperparams {\r\n",
        "          activation: RELU_6,\r\n",
        "          regularizer {\r\n",
        "            l2_regularizer {\r\n",
        "            # weight: 0.00004\r\n",
        "            weight: 0.001 # higher regularizition to counter overfitting\r\n",
        "          }\r\n",
        "          }\r\n",
        "          initializer {\r\n",
        "            truncated_normal_initializer {\r\n",
        "              stddev: 0.03\r\n",
        "              mean: 0.0\r\n",
        "            }\r\n",
        "          }\r\n",
        "          batch_norm {\r\n",
        "            train: true,\r\n",
        "            scale: true,\r\n",
        "            center: true,\r\n",
        "            decay: 0.9997,\r\n",
        "            epsilon: 0.001,\r\n",
        "          }\r\n",
        "        }\r\n",
        "      }\r\n",
        "    }\r\n",
        "    feature_extractor {\r\n",
        "      type: 'ssd_mobilenet_v2'\r\n",
        "      min_depth: 16\r\n",
        "      depth_multiplier: 1.0\r\n",
        "      conv_hyperparams {\r\n",
        "        activation: RELU_6,\r\n",
        "        regularizer {\r\n",
        "          l2_regularizer {\r\n",
        "            # weight: 0.00004\r\n",
        "            weight: 0.001 # higher regularizition to counter overfitting\r\n",
        "          }\r\n",
        "        }\r\n",
        "        initializer {\r\n",
        "          truncated_normal_initializer {\r\n",
        "            stddev: 0.03\r\n",
        "            mean: 0.0\r\n",
        "          }\r\n",
        "        }\r\n",
        "        batch_norm {\r\n",
        "          train: true,\r\n",
        "          scale: true,\r\n",
        "          center: true,\r\n",
        "          decay: 0.9997,\r\n",
        "          epsilon: 0.001,\r\n",
        "        }\r\n",
        "      }\r\n",
        "    }\r\n",
        "    loss {\r\n",
        "      classification_loss {\r\n",
        "        weighted_sigmoid {\r\n",
        "        }\r\n",
        "      }\r\n",
        "      localization_loss {\r\n",
        "        weighted_smooth_l1 {\r\n",
        "        }\r\n",
        "      }\r\n",
        "      hard_example_miner {\r\n",
        "        num_hard_examples: 3000 \r\n",
        "        iou_threshold: 0.95\r\n",
        "        loss_type: CLASSIFICATION\r\n",
        "        max_negatives_per_positive: 3\r\n",
        "        min_negatives_per_image: 3\r\n",
        "      }\r\n",
        "      classification_weight: 1.0\r\n",
        "      localization_weight: 1.0\r\n",
        "    }\r\n",
        "    normalize_loss_by_num_matches: true\r\n",
        "    post_processing {\r\n",
        "      batch_non_max_suppression {\r\n",
        "        score_threshold: 1e-8\r\n",
        "        iou_threshold: 0.6\r\n",
        "        \r\n",
        "        #adjust this to the max number of objects per class. \r\n",
        "        # ex, in my case, i have one pistol in most of the images.\r\n",
        "        # . there are some images with more than one up to 16.\r\n",
        "        max_detections_per_class: 16\r\n",
        "        # max number of detections among all classes. I have 1 class only so\r\n",
        "        max_total_detections: 16\r\n",
        "      }\r\n",
        "      score_converter: SIGMOID\r\n",
        "    }\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "train_config: {\r\n",
        "  batch_size: 16 # training batch size\r\n",
        "  optimizer {\r\n",
        "    rms_prop_optimizer: {\r\n",
        "      learning_rate: {\r\n",
        "        exponential_decay_learning_rate {\r\n",
        "          initial_learning_rate: 0.003\r\n",
        "          decay_steps: 800720\r\n",
        "          decay_factor: 0.95\r\n",
        "        }\r\n",
        "      }\r\n",
        "      momentum_optimizer_value: 0.9\r\n",
        "      decay: 0.9\r\n",
        "      epsilon: 1.0\r\n",
        "    }\r\n",
        "  }\r\n",
        "\r\n",
        "  #the path to the pretrained model. \r\n",
        "  fine_tune_checkpoint: \"/gdrive/My Drive/object_detection/knife_detection/models/research/pretrained_model/model.ckpt\"\r\n",
        "  fine_tune_checkpoint_type:  \"detection\"\r\n",
        "  # Note: The below line limits the training process to 200K steps, which we\r\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\r\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\r\n",
        "  # never decay). Remove the below line to train indefinitely.\r\n",
        "  num_steps: 200000 \r\n",
        "  \r\n",
        "\r\n",
        "  #data augmentaion is done here, you can remove or add more.\r\n",
        "  # They will help the model generalize but the training time will increase greatly by using more data augmentation.\r\n",
        "  # Check this link to add more image augmentation: https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto\r\n",
        "  \r\n",
        "  data_augmentation_options {\r\n",
        "    random_horizontal_flip {\r\n",
        "    }\r\n",
        "  }\r\n",
        "  data_augmentation_options {\r\n",
        "    random_adjust_contrast {\r\n",
        "    }\r\n",
        "  }\r\n",
        "  data_augmentation_options {\r\n",
        "    ssd_random_crop {\r\n",
        "    }\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "train_input_reader: {\r\n",
        "  tf_record_input_reader {\r\n",
        "    #path to the training TFRecord\r\n",
        "    input_path: \"/gdrive/My Drive/object_detection/knife_detection/data/train_labels.record\"\r\n",
        "  }\r\n",
        "  #path to the label map \r\n",
        "  label_map_path: \"/gdrive/My Drive/object_detection/knife_detection/data/label_map.pbtxt\"\r\n",
        "}\r\n",
        "\r\n",
        "eval_config: {\r\n",
        "  # the number of images in your \"testing\" data (was 600 but we removed one above :) )\r\n",
        "  num_examples: 400\r\n",
        "  # the number of images to display in Tensorboard while training\r\n",
        "  num_visualizations: 20\r\n",
        "\r\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\r\n",
        "  # Remove the below line to evaluate indefinitely.\r\n",
        "  #max_evals: 10\r\n",
        "}\r\n",
        "\r\n",
        "eval_input_reader: {\r\n",
        "  tf_record_input_reader {\r\n",
        "      \r\n",
        "    #path to the testing TFRecord\r\n",
        "    input_path: \"/gdrive/My Drive/object_detection/knife_detection/data/test_labels.record\"\r\n",
        "  }\r\n",
        "  #path to the label map \r\n",
        "  label_map_path: \"/gdrive/My Drive/object_detection/knife_detection/data/label_map.pbtxt\"\r\n",
        "  shuffle: false\r\n",
        "  num_readers: 1\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weK8CxXaTMiq"
      },
      "source": [
        "# where the model will be saved at each checkpoint while training \r\n",
        "model_dir = 'training/'\r\n",
        "\r\n",
        "# Optionally: remove content in output model directory to fresh start.\r\n",
        "!rm -rf {model_dir}\r\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnyInpLQbV2Q"
      },
      "source": [
        "##Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-e8j-loTPj0"
      },
      "source": [
        "#downlaoding ngrok to be able to access tensorboard on google colab\r\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\r\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HEMBP3HTTEk"
      },
      "source": [
        "#the logs that are created while training \r\n",
        "LOG_DIR = '/gdrive/My\\ Drive/object_detection/knife_detection/models/research/object_detection/training/'\r\n",
        "get_ipython().system_raw(\r\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\r\n",
        "    .format(LOG_DIR)\r\n",
        ")\r\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZO_Xnu8TW7M"
      },
      "source": [
        "#The link to tensorboard.\r\n",
        "#works after the training starts.\r\n",
        "\r\n",
        "### note: if you didnt get a link as output, rerun this cell and the one above\r\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\r\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9RVjUWxx_mF"
      },
      "source": [
        "#an ulternative way to open tensorboard in the same tab\r\n",
        "# %load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBHp6M_zgjp4"
      },
      "source": [
        "# %tensorboard --logdir /gdrive/My\\ Drive/object_detection/knife_detection/models/research/object_detection/training/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHpcoj0Wblzh"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHWy7x5iTdgr"
      },
      "source": [
        "#check the model main file that is provided by the tf model\r\n",
        "%cd /gdrive/My\\ Drive/object_detection/knife_detection/models/research/object_detection\r\n",
        "!cat model_main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrKUmKbSTfV9"
      },
      "source": [
        "!python3 model_main.py \\\r\n",
        "    --pipeline_config_path={model_pipline}\\\r\n",
        "    --model_dir={model_dir} \\\r\n",
        "    --alsologtostderr \\"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
